<img src="http://upload-images.jianshu.io/upload_images/15675864-952291e89189c8a8.jpg">



记录python学习中遇到的一些心得：

[TOC]

***

# python多进程

Python中的多进程是通过multiprocessing包来实现的，和多线程的threading.Thread差不多，它可以利用multiprocessing.Process对象来创建一个进程对象。这个进程对象的方法和线程对象的方法差不多也有start(), run(), join()等方法，其中有一个方法不同Thread线程对象中的守护线程方法是setDeamon，而Process进程对象的守护进程是通过设置daemon属性来完成的。

与多线程的共享式内存不同，由于各个进程都是相互独立的，因此进程间通信再多进程中扮演这非常重要的角色，Python中我们可以使用multiprocessing模块中的`pipe`、`queue`、`Array`、`Value`等等工具来实现进程间通讯和数据共享，但是在编写起来仍然具有很大的不灵活性。

![img](res/Python%20Base/13088413-c5ced16c6b0c2d29.webp)

- **同步**就是指一个进程在执行某个请求的时候,若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去；

- **异步**是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率。

  - **对于IO密集型任务,**python的多线程能够节省时间.
    **对于计算密集型任务,**Python的多线程并没有用处.

  - 如果程序是属于 CPU 密集型，建议使用多进程。而多线程就更适合应用于 I/O 密集型程序。

  - #####  python使用多核，即开多个进程。

    - 方法一: 协程+多进程。使用方法简单，效率还可以，一般使用该方法。
      协程yield是你自己写的，是自己定义什么时候切换进程。
    - 方法二：IO多路复用。使用复杂，但效率很高。不常用。



```powershell
创建管理进程模块：

Process(用于创建进程):通过创建一个Process对象然后调用它的start()方法来生成进程。Process遵循threading.Thread的API。
Pool(用于创建进程管理池)：可以创建一个进程池，该进程将执行与Pool该类一起提交给它的任务，当子进程较多需要管理时使用。
Queue（用于进程通信，资源共享）：进程间通信，保证进程安全。
Value，Array（用于进程通信，资源共享）：
Pipe（用于管道通信）：管道操作。
Manager（用于资源共享）：创建进程间共享的数据，包括在不同机器上运行的进程之间的网络共享。

同步子进程模块：

Condition
Event：用来实现进程间同步通信。
Lock：当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突。
RLock
Semaphore：用来控制对共享资源的访问数量，例如池的最大连接数。
```

# python多线程低效原因

**GIL**的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。

而目前 Python 的解释器有多种，例如：

- **CPython**：CPython 是用C语言实现的 Python 解释器。 作为官方实现，它是最广泛使用的 Python 解释器。
- **PyPy**：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端（C, CLI, JVM）。PyPy 旨在提高性能，同时保持最大兼容性（参考 CPython 的实现）。
- **Jython**：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM (Java Virtual Machine) 上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类。
- **IronPython**：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言。

GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的。

每次释放 GIL锁，线程进行锁竞争、切换线程，会消耗资源。这就导致打印线程执行时长，会发现耗时更长的原因。

并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因。

# python多进程实现方式

## 多进程之Process

```python
from multiprocessing import  Process

def fun1(name):
    print('测试%s多进程' %name)

if __name__ == '__main__':
    process_list = []
    for i in range(5):  #开启5个子进程执行fun1函数
        p = Process(target=fun1,args=('Python',)) #实例化进程对象
        p.start()
        process_list.append(p)

    for i in process_list:
        p.join()

    print('结束测试')
```

```powershell
测试Python多进程
测试Python多进程
测试Python多进程
测试Python多进程
测试Python多进程
结束测试

Process finished with exit code 0
```

```shell
上面的代码开启了5个子进程去执行函数，我们可以观察结果，是同时打印的，这里实现了真正的并行操作，就是多个CPU同时执行任务。我们知道进程是python中最小的资源分配单元，也就是进程中间的数据，内存是不共享的，每启动一个进程，都要独立分配资源和拷贝访问的数据，所以进程的启动和销毁的代价是比较大了，所以在实际中使用多进程，要根据服务器的配置来设定。
```



## 多进程之继承Process

```python
from multiprocessing import  Process

class MyProcess(Process): #继承Process类
    def __init__(self,name):
        super(MyProcess,self).__init__()
        self.name = name

    def run(self):
        print('测试%s多进程' % self.name)


if __name__ == '__main__':
    process_list = []
    for i in range(5):  #开启5个子进程执行fun1函数
        p = MyProcess('Python') #实例化进程对象
        p.start()
        process_list.append(p)

    for i in process_list:
        p.join()

    print('结束测试')
```

```powershell
测试Python多进程
测试Python多进程
测试Python多进程
测试Python多进程
测试Python多进程
结束测试

Process finished with exit code 0
```

```
通过类继承的方法来实现的，python多进程的第二种实现方式也是一样的,效果和第一种方式一样。
```



## Process类的其他方法

```powershell
构造方法：

Process([group [, target [, name [, args [, kwargs]]]]])
　　group: 线程组 
　　target: 要执行的方法
　　name: 进程名
　　args/kwargs: 要传入方法的参数

实例方法：
　　is_alive()：返回进程是否在运行,bool类型。
　　join([timeout])：阻塞当前上下文环境的进程程，直到调用此方法的进程终止或到达指定的timeout（可选参数）。
　　start()：进程准备就绪，等待CPU调度
　　run()：strat()调用run方法，如果实例进程时未制定传入target，这star执行t默认run()方法。
　　terminate()：不管任务是否完成，立即停止工作进程

属性：
　　daemon：和线程的setDeamon功能一样
　　name：进程名字
　　pid：进程号
```



## 多进程之进程池

```python
# apply_async：异步
from  multiprocessing import Pool,cpu_count
import os, time, random

def fun1(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))
    return f'{name}: {os.getpid()}'

if __name__=='__main__':
    results = []
    pool = Pool(cpu_count()-1)

    for i in range(4):
        results.append(pool.apply_async(func=fun1, args=(i,)))

    pool.close()
    pool.join()
    print()
    for result in results:
        print(result.get())
    print('All Done!!!')

    print('结束测试')
```

```powershell
Run task 0 (30716)...
Run task 1 (15020)...
Run task 2 (23200)...
Run task 3 (5884)...
Task 0 runs 1.34 seconds.
Task 2 runs 1.53 seconds.
Task 1 runs 1.88 seconds.
Task 3 runs 2.48 seconds.

0: 30716
1: 15020
2: 23200
3: 5884
All Done!!!
结束测试

Process finished with exit code 0
```

```
对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。
```

```python
# map_async：异步
from multiprocessing import Pool, cpu_count, Manager
from functools import partial

def job(data, mgrDicTask, lock):
    res = f'a+b = {data[0] + data[1]}'
    lock.acquire()
    # Manager对象无法监测到它引用的可变对象值的修改，需要通过触发__setitem__方法来让它获得通知
    tempDic = list(mgrDicTask['result'])
    tempDic.append(res)
    mgrDicTask['result'] = tempDic
    lock.release()
    return res

if __name__ == "__main__":
    data = [[2, 3], [3, 4], [2, 5]]
    pool = Pool(processes=cpu_count() - 1)

    mgr = Manager()
    lock = mgr.Lock()
    mgrDicTask = mgr.dict()
    mgrDicTask['result'] = []

    fun = partial(job, mgrDicTask=mgrDicTask, lock=lock)
    pool.map_async(fun, data)
    pool.close()
    pool.join()

    print(mgrDicTask['result'])
    print('All Done!!!')
```

```powershell
['a+b = 7', 'a+b = 7', 'a+b = 5']
All Done!!!

Process finished with exit code 0
```

## 多进程之进程池的另一种方式

```python
# 进程池的另外一种创建方式，跟线程池的创建方式一样。其方法等也相同。
def process_pool_test(url_list):
    book_list = []
    # 创建进程池
    pool = ProcessPoolExecutor(max_workers=20)
    start = time.time()
    for url in url_list:
        time.sleep(0.5)
        result = pool.submit(get_book_info, url)
        book_list.append(result)

    pool.shutdown()
    print('time: ', time.time() - start)

    book_name_list = []
    author_list = []
    author_info_list = []
    print('book_list: ', len(book_list))
    for future in book_list:
        book_name_list.extend(future.result()['name'])
        author_list.extend(future.result()['author'])
        author_info_list.extend(future.result()['info'])

    ExcelUtils.write_data_to_excel('bookInfo', book_name_list, author_list, author_info_list)


if __name__ == '__main__':
    sys.setrecursionlimit(10000)
    url_list = ['https://www.edge.org/library']
    for i in range(1, 52):
        url_list.append('https://www.edge.org/library?page=%s' % i)

    thread_pool_test(url_list)
```



# python多进程通信

```
进程是系统独立调度核分配系统资源（CPU、内存）的基本单位，进程之间是相互独立的，每启动一个新的进程相当于把数据进行了一次克隆，子进程里的数据修改无法影响到主进程中的数据，不同子进程之间的数据也不能共享，这是多进程在使用中与多线程最明显的区别。但是难道Python多进程中间难道就是孤立的吗？当然不是，python也提供了多种方法实现了多进程中间的通信和数据共享（可以修改一份数据）
```

## 进程对列Queue

```python
# Queue在多线程中也说到过，在生成者消费者模式中使用，是线程安全的，是生产者和消费者中间的数据管道，那在python多进程中，它其实就是进程之间的数据管道，实现进程通信。
from multiprocessing import Process,Queue


def fun1(q,i):
    print('子进程%s 开始put数据' %i)
    q.put('我是%s 通过Queue通信' %i)

if __name__ == '__main__':
    q = Queue()

    process_list = []
    for i in range(3):
        p = Process(target=fun1,args=(q,i,))  #注意args里面要把q对象传给我们要执行的方法，这样子进程才能和主进程用Queue来通信
        p.start()
        process_list.append(p)

    for i in process_list:
        p.join()

    print('主进程获取Queue数据')
    print(q.get())
    print(q.get())
    print(q.get())
    print('结束测试')
```

```powershell
子进程0 开始put数据
子进程1 开始put数据
子进程2 开始put数据
主进程获取Queue数据
我是0 通过Queue通信
我是1 通过Queue通信
我是2 通过Queue通信
结束测试

Process finished with exit code 0
```

```powershell
上面的代码结果可以看到我们主进程中可以通过Queue获取子进程中put的数据，实现进程间的通信。
```



## 管道Pipe

```python
# Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像。pipe() 返回两个连接对象分别表示管道的两端，每端都有send() 和recv()函数。

# 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据。
# 管道是数据不安全的，多个进程同时收发数据可道引起数据异常，这时候就应该配合锁使用。
from multiprocessing import Process, Pipe

def fun1(conn):
    print('子进程发送消息：')
    conn.send('你好主进程')
    print('子进程接受消息：')
    print(conn.recv())
    conn.close()

if __name__ == '__main__':
    conn1, conn2 = Pipe() #关键点，pipe实例化生成一个双向管
    p = Process(target=fun1, args=(conn2,)) #conn2传给子进程
    p.start()
    print('主进程接受消息：')
    print(conn1.recv())
    print('主进程发送消息：')
    conn1.send("你好子进程")
    p.join()
    print('结束测试')
```

```powershell
主进程接受消息：
子进程发送消息：
子进程接受消息：
你好主进程
主进程发送消息：
你好子进程
结束测试

Process finished with exit code 0
```

```powershell
上面可以看到主进程和子进程可以相互发送消息
```

## Managers

```
Queue和Pipe只是实现了数据交互，并没实现数据共享，即一个进程去更改另一个进程的数据。那么就要用到Managers
```

```python
from multiprocessing import Process, Manager

def fun1(dic,lis,index):
    dic[index] = 'a'
    dic['2'] = 'b'    
    lis.append(index)    #[0,1,2,3,4,0,1,2,3,4,5,6,7,8,9]
    #print(l)

if __name__ == '__main__':
    with Manager() as manager:
        dic = manager.dict()#注意字典的声明方式，不能直接通过{}来定义
        l = manager.list(range(5))#[0,1,2,3,4]

        process_list = []
        for i in range(10):
            p = Process(target=fun1, args=(dic,l,i))
            p.start()
            process_list.append(p)

        for res in process_list:
            res.join()
        print(dic)
        print(l)
```

```powershell
{0: 'a', '2': 'b', 3: 'a', 1: 'a', 2: 'a', 4: 'a', 5: 'a', 7: 'a', 6: 'a', 8: 'a', 9: 'a'}
[0, 1, 2, 3, 4, 0, 3, 1, 2, 4, 5, 7, 6, 8, 9]
```

```powershell
可以看到主进程定义了一个字典和一个列表，在子进程中，可以添加和修改字典的内容，在列表中插入新的数据，实现进程间的数据共享，即可以共同修改同一份数据。
```



4. [内容提取神器 beautiful Soup 的用法](https://www.jianshu.com/p/865d73b70ec7)

   

5. 
